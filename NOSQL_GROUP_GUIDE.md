# NoSQLç»„å®æ–½æŒ‡å— - MongoDB + Redisæ–¹æ¡ˆ


## ğŸ“‹ ä»»åŠ¡æ¸…å•

- [ ] Task 1: å®‰è£…MongoDBå’ŒRedisï¼ˆ30åˆ†é’Ÿï¼‰
- [ ] Task 2: è®¾è®¡æ–‡æ¡£ç»“æ„ï¼ˆ1å°æ—¶ï¼‰
- [ ] Task 3: åˆ›å»ºç´¢å¼•ï¼ˆ30åˆ†é’Ÿï¼‰
- [ ] Task 4: ç¼–å†™æ•°æ®åŠ è½½è„šæœ¬ï¼ˆ2å°æ—¶ï¼‰
- [ ] Task 5: å®ç°5ä¸ªæŸ¥è¯¢ï¼ˆ2å°æ—¶ï¼‰
- [ ] Task 6: é…ç½®Redisç¼“å­˜ï¼ˆ1å°æ—¶ï¼‰
- [ ] Task 7: æµ‹è¯•å’Œæ–‡æ¡£ï¼ˆ1å°æ—¶ï¼‰

---

## ğŸ› ï¸ Task 1: å®‰è£…MongoDBå’ŒRedis

### æ–¹æ³•1ï¼šDocker Composeï¼ˆæ¨èï¼‰

åˆ›å»º `docker-compose.yml`:

```yaml
version: '3.8'

services:
  mongodb:
    image: mongo:7.0
    container_name: token-analyzer-mongo
    ports:
      - "27017:27017"
    environment:
      MONGO_INITDB_ROOT_USERNAME: admin
      MONGO_INITDB_ROOT_PASSWORD: yourpassword
      MONGO_INITDB_DATABASE: token_analyzer
    volumes:
      - mongodb_data:/data/db

  redis:
    image: redis:7-alpine
    container_name: token-analyzer-redis
    ports:
      - "6379:6379"
    command: redis-server --appendonly yes
    volumes:
      - redis_data:/data

volumes:
  mongodb_data:
  redis_data:
```

å¯åŠ¨:

```bash
docker-compose up -d

# éªŒè¯
docker ps
docker exec -it token-analyzer-mongo mongosh -u admin -p yourpassword
docker exec -it token-analyzer-redis redis-cli ping
```

### æ–¹æ³•2ï¼šæœ¬åœ°å®‰è£…

```bash
# Ubuntu/Debian
sudo apt install mongodb redis-server

# macOS
brew install mongodb-community redis
brew services start mongodb-community
brew services start redis
```

---

## ğŸ“Š Task 2: è®¾è®¡æ–‡æ¡£ç»“æ„

NoSQLçš„æ ¸å¿ƒä¼˜åŠ¿æ˜¯**å†…åµŒæ–‡æ¡£**å’Œ**çµæ´»schema**ã€‚

### é›†åˆç»“æ„è®¾è®¡

#### 1. tokensï¼ˆä»£å¸é›†åˆï¼‰

```javascript
{
  _id: ObjectId("..."),
  address: "0x...",  // ç´¢å¼•
  symbol: "ETH",
  name: "Ethereum",
  chain: "ETH",
  is_proxy: false,
  is_upgradeable: false,

  // å†…åµŒæœ€æ–°ä»·æ ¼ï¼ˆæé«˜æŸ¥è¯¢é€Ÿåº¦ï¼‰
  latest_price: {
    dex_price: 1800.50,
    cex_price: 1801.20,
    tvl: 1500000000,
    timestamp: ISODate("2025-01-09T...")
  },

  // å†…åµŒæœ€æ–°è¯„åˆ†
  latest_score: {
    score: 85,
    factors: {
      liquidity_score: 90,
      holder_score: 75,
      dev_score: 88
    },
    timestamp: ISODate("2025-01-09T...")
  },

  // å†…åµŒé¡¹ç›®ä¿¡æ¯
  project: {
    name: "Ethereum",
    github_repo: "https://github.com/ethereum/go-ethereum",
    stars: 45000,
    commit_count_7d: 50,
    last_commit_at: ISODate("2025-01-09T...")
  },

  created_at: ISODate("2025-01-01T..."),
  updated_at: ISODate("2025-01-09T...")
}
```

**è®¾è®¡ç†ç”±**ï¼š
- å°†å¸¸ç”¨çš„æœ€æ–°æ•°æ®**å†…åµŒ**åˆ°ä¸»æ–‡æ¡£ï¼Œé¿å…JOIN
- ä¸€æ¬¡æŸ¥è¯¢å°±èƒ½è·å–ä»£å¸çš„å…¨éƒ¨å…³é”®ä¿¡æ¯

#### 2. dex_pricesï¼ˆDEXä»·æ ¼æ—¶åºæ•°æ®ï¼‰

```javascript
{
  _id: ObjectId("..."),
  token_address: "0x...",  // ç´¢å¼•
  price: 1800.50,
  tvl: 1500000000,
  liquidity_depth: 5000000,
  volume_24h: 2000000,
  timestamp: ISODate("2025-01-09T..."),  // ç´¢å¼•

  // å¯é€‰ï¼šå°æ—¶èšåˆæ•°æ®ï¼ˆé¢„èšåˆä¼˜åŒ–ï¼‰
  hour: "2025-01-09T10:00:00Z"
}
```

#### 3. cex_pricesï¼ˆCEXä»·æ ¼æ•°æ®ï¼‰

```javascript
{
  _id: ObjectId("..."),
  token_symbol: "ETH",  // ç´¢å¼•
  exchange: "Binance",
  spot_price: 1801.20,
  funding_rate: 0.0001,
  volume_24h: 5000000,
  timestamp: ISODate("2025-01-09T...")  // ç´¢å¼•
}
```

#### 4. token_holdersï¼ˆæŒä»“æ•°æ®ï¼‰

```javascript
{
  _id: ObjectId("..."),
  token_address: "0x...",  // ç´¢å¼•
  holders: [  // å†…åµŒæ•°ç»„ï¼ˆæ‰€æœ‰æŒä»“è€…ï¼‰
    {
      address: "0x...",
      balance: "1000000000000000000",
      percentage: 15.5,
      rank: 1
    },
    {
      address: "0x...",
      balance: "500000000000000000",
      percentage: 8.2,
      rank: 2
    }
    // ... Top 20
  ],
  snapshot_date: ISODate("2025-01-09"),
  top10_concentration: 65.8  // é¢„è®¡ç®—
}
```

**è®¾è®¡ç†ç”±**ï¼š
- å°†æ‰€æœ‰æŒä»“è€…å­˜ä¸º**æ•°ç»„**ï¼Œä¸€æ¬¡æŸ¥è¯¢è·å–å…¨éƒ¨æ•°æ®
- é¢„è®¡ç®— `top10_concentration`ï¼Œé¿å…å®æ—¶èšåˆ

#### 5. token_scoresï¼ˆè¯„åˆ†å†å²ï¼‰

```javascript
{
  _id: ObjectId("..."),
  token_address: "0x...",  // ç´¢å¼•
  score: 85,
  score_factors: {
    liquidity_score: 90,
    holder_score: 75,
    dev_score: 88
  },
  timestamp: ISODate("2025-01-09T...")  // ç´¢å¼•
}
```

#### 6. alertsï¼ˆé¢„è­¦è®°å½•ï¼‰

```javascript
{
  _id: ObjectId("..."),
  token_address: "0x...",  // ç´¢å¼•
  alert_type: "LIQUIDITY_DROP",
  severity: "HIGH",
  message: "TVL dropped by 25% in the last hour",
  timestamp: ISODate("2025-01-09T..."),  // ç´¢å¼•
  acknowledged: false
}
```

---

## ğŸ”§ Task 3: åˆ›å»ºç´¢å¼•

åˆ›å»º `nosql_indexes.js`:

```javascript
// è¿æ¥åˆ°æ•°æ®åº“
use token_analyzer;

// 1. tokensé›†åˆç´¢å¼•
db.tokens.createIndex({ "address": 1 }, { unique: true });
db.tokens.createIndex({ "symbol": 1 });
db.tokens.createIndex({ "latest_score.score": -1 });
db.tokens.createIndex({ "is_proxy": 1, "is_upgradeable": 1 });

// 2. dex_pricesç´¢å¼•ï¼ˆæ—¶åºæ•°æ®ï¼‰
db.dex_prices.createIndex({ "token_address": 1, "timestamp": -1 });
db.dex_prices.createIndex({ "timestamp": -1 });
db.dex_prices.createIndex({ "hour": 1 });  // ç”¨äºå°æ—¶èšåˆ

// 3. cex_pricesç´¢å¼•
db.cex_prices.createIndex({ "token_symbol": 1, "timestamp": -1 });
db.cex_prices.createIndex({ "exchange": 1, "timestamp": -1 });

// 4. token_holdersç´¢å¼•
db.token_holders.createIndex({ "token_address": 1, "snapshot_date": -1 }, { unique: true });
db.token_holders.createIndex({ "snapshot_date": -1 });
db.token_holders.createIndex({ "holders.address": 1 });  // å¤šé”®ç´¢å¼•

// 5. token_scoresç´¢å¼•
db.token_scores.createIndex({ "token_address": 1, "timestamp": -1 });
db.token_scores.createIndex({ "score": -1, "timestamp": -1 });

// 6. alertsç´¢å¼•
db.alerts.createIndex({ "token_address": 1, "timestamp": -1 });
db.alerts.createIndex({ "severity": 1, "timestamp": -1 });
db.alerts.createIndex({ "acknowledged": 1, "timestamp": -1 });

print("âœ“ æ‰€æœ‰ç´¢å¼•åˆ›å»ºå®Œæˆ");
```

æ‰§è¡Œç´¢å¼•åˆ›å»º:

```bash
docker exec -i token-analyzer-mongo mongosh -u admin -p yourpassword token_analyzer < nosql_indexes.js

# éªŒè¯ç´¢å¼•
docker exec -it token-analyzer-mongo mongosh -u admin -p yourpassword
use token_analyzer
db.tokens.getIndexes()
```

---

## ğŸ Task 4: æ•°æ®åŠ è½½è„šæœ¬

åˆ›å»º `nosql_data_loader.py`:

```python
from pymongo import MongoClient
import redis
from datetime import datetime, timedelta
import random
from faker import Faker
import json

fake = Faker()

# è¿æ¥MongoDB
mongo_client = MongoClient(
    "mongodb://admin:yourpassword@localhost:27017/",
    serverSelectionTimeoutMS=5000
)
db = mongo_client["token_analyzer"]

# è¿æ¥Redis
redis_client = redis.Redis(host='localhost', port=6379, decode_responses=True)

print("å¼€å§‹ç”Ÿæˆæ•°æ®...")

# 1. ç”Ÿæˆ100ä¸ªä»£å¸
print("ç”Ÿæˆtokensæ•°æ®...")
tokens = []
token_addresses = []

for i in range(100):
    address = f"0x{fake.hexify(text='^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^')}"
    token_addresses.append(address)

    token = {
        "address": address,
        "symbol": fake.cryptocurrency_code(),
        "name": fake.cryptocurrency_name(),
        "chain": "ETH",
        "is_proxy": random.choice([True, False]),
        "is_upgradeable": random.choice([True, False]),
        "latest_price": {
            "dex_price": random.uniform(0.1, 10000),
            "tvl": random.uniform(100000, 10000000),
            "timestamp": datetime.now()
        },
        "latest_score": {
            "score": random.randint(30, 95),
            "factors": {
                "liquidity_score": random.randint(50, 100),
                "holder_score": random.randint(40, 90),
                "dev_score": random.randint(30, 80)
            },
            "timestamp": datetime.now()
        },
        "project": {
            "name": f"{fake.cryptocurrency_code()} Project",
            "github_repo": f"https://github.com/{fake.user_name()}/{fake.word()}",
            "stars": random.randint(10, 10000),
            "commit_count_7d": random.randint(0, 100),
            "last_commit_at": datetime.now() - timedelta(days=random.randint(0, 30))
        },
        "created_at": datetime.now(),
        "updated_at": datetime.now()
    }
    tokens.append(token)

result = db.tokens.insert_many(tokens)
print(f"âœ“ æ’å…¥äº† {len(result.inserted_ids)} ä¸ªä»£å¸")

# ç¼“å­˜åˆ°Redisï¼ˆç‚¹æŸ¥è¯¢ä¼˜åŒ–ï¼‰
print("ç¼“å­˜ä»£å¸æ•°æ®åˆ°Redis...")
for token in tokens:
    cache_key = f"token:{token['address']}:latest"
    cache_data = {
        "symbol": token["symbol"],
        "name": token["name"],
        "price": token["latest_price"]["dex_price"],
        "tvl": token["latest_price"]["tvl"],
        "score": token["latest_score"]["score"]
    }
    redis_client.setex(cache_key, 300, json.dumps(cache_data))  # 5åˆ†é’ŸTTL

print("âœ“ Redisç¼“å­˜å·²æ›´æ–°")

# 2. ç”ŸæˆDEXä»·æ ¼æ•°æ®ï¼ˆæ¯ä¸ªä»£å¸1000æ¡ = 10ä¸‡æ¡ï¼‰
print("ç”Ÿæˆdex_pricesæ•°æ®ï¼ˆè¿™ä¼šèŠ±å‡ åˆ†é’Ÿï¼‰...")
batch_size = 1000
dex_prices = []

for address in token_addresses:
    base_price = random.uniform(0.1, 10000)
    for i in range(1000):
        timestamp = datetime.now() - timedelta(minutes=i)
        price = base_price * (1 + random.uniform(-0.05, 0.05))
        dex_prices.append({
            "token_address": address,
            "price": price,
            "tvl": random.uniform(100000, 10000000),
            "liquidity_depth": random.uniform(50000, 5000000),
            "volume_24h": random.uniform(10000, 1000000),
            "timestamp": timestamp,
            "hour": timestamp.replace(minute=0, second=0, microsecond=0)
        })

        # æ‰¹é‡æ’å…¥
        if len(dex_prices) >= batch_size:
            db.dex_prices.insert_many(dex_prices)
            print(f"  å·²æ’å…¥ {len(dex_prices)} æ¡DEXä»·æ ¼...")
            dex_prices = []

# æ’å…¥å‰©ä½™æ•°æ®
if dex_prices:
    db.dex_prices.insert_many(dex_prices)

print(f"âœ“ æ’å…¥äº†çº¦ 100,000 æ¡DEXä»·æ ¼æ•°æ®")

# 3. ç”ŸæˆCEXä»·æ ¼æ•°æ®
print("ç”Ÿæˆcex_pricesæ•°æ®...")
exchanges = ['Binance', 'OKX', 'Coinbase', 'Bybit']
cex_prices = []

for token in tokens[:50]:  # å‰50ä¸ªä»£å¸
    symbol = token["symbol"]
    for i in range(500):
        timestamp = datetime.now() - timedelta(minutes=i*2)
        cex_prices.append({
            "token_symbol": symbol,
            "exchange": random.choice(exchanges),
            "spot_price": random.uniform(0.1, 10000),
            "funding_rate": random.uniform(-0.0001, 0.0001),
            "volume_24h": random.uniform(10000, 1000000),
            "timestamp": timestamp
        })

db.cex_prices.insert_many(cex_prices)
print(f"âœ“ æ’å…¥äº† {len(cex_prices)} æ¡CEXä»·æ ¼æ•°æ®")

# 4. ç”ŸæˆæŒä»“æ•°æ®
print("ç”Ÿæˆtoken_holdersæ•°æ®...")
holders_docs = []

for address in token_addresses:
    # æ¯ä¸ªä»£å¸ç”Ÿæˆ20ä¸ªæŒä»“è€…
    holders = []
    for rank in range(1, 21):
        holders.append({
            "address": f"0x{fake.hexify(text='^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^')}",
            "balance": str(random.randint(1000000, 100000000)),
            "percentage": random.uniform(0.5, 20.0),
            "rank": rank
        })

    # è®¡ç®—Top 10é›†ä¸­åº¦
    top10_concentration = sum(h["percentage"] for h in holders[:10])

    holders_docs.append({
        "token_address": address,
        "holders": holders,
        "top10_concentration": top10_concentration,
        "snapshot_date": datetime.now()
    })

db.token_holders.insert_many(holders_docs)
print(f"âœ“ æ’å…¥äº† {len(holders_docs)} ä¸ªä»£å¸çš„æŒä»“æ•°æ®")

# 5. ç”Ÿæˆè¯„åˆ†æ•°æ®
print("ç”Ÿæˆtoken_scoresæ•°æ®...")
scores = []

for address in token_addresses:
    for i in range(10):  # æ¯ä¸ªä»£å¸10æ¡å†å²è¯„åˆ†
        timestamp = datetime.now() - timedelta(hours=i*24)
        scores.append({
            "token_address": address,
            "score": random.randint(30, 95),
            "score_factors": {
                "liquidity_score": random.randint(50, 100),
                "holder_score": random.randint(40, 90),
                "dev_score": random.randint(30, 80)
            },
            "timestamp": timestamp
        })

db.token_scores.insert_many(scores)
print(f"âœ“ æ’å…¥äº† {len(scores)} æ¡è¯„åˆ†æ•°æ®")

# 6. ç”Ÿæˆé¢„è­¦æ•°æ®
print("ç”Ÿæˆalertsæ•°æ®...")
alert_types = ['LIQUIDITY_DROP', 'PRICE_SPIKE', 'WHALE_MOVEMENT', 'RUG_PULL_RISK']
severities = ['LOW', 'MEDIUM', 'HIGH', 'CRITICAL']
alerts = []

for address in random.sample(token_addresses, 30):
    for i in range(5):
        alerts.append({
            "token_address": address,
            "alert_type": random.choice(alert_types),
            "severity": random.choice(severities),
            "message": f"Alert: {fake.sentence()}",
            "timestamp": datetime.now() - timedelta(hours=i*6),
            "acknowledged": False
        })

db.alerts.insert_many(alerts)
print(f"âœ“ æ’å…¥äº† {len(alerts)} æ¡é¢„è­¦")

print("\nâœ… NoSQLæ•°æ®åŠ è½½å®Œæˆï¼")
print(f"æ€»è®¡:")
print(f"  - {len(tokens)} ä¸ªä»£å¸ (MongoDB + Redisç¼“å­˜)")
print(f"  - ~100,000 æ¡DEXä»·æ ¼è®°å½•")
print(f"  - {len(cex_prices)} æ¡CEXä»·æ ¼è®°å½•")
print(f"  - {len(holders_docs)} ä¸ªä»£å¸çš„æŒä»“è®°å½•")
print(f"  - {len(scores)} æ¡è¯„åˆ†è®°å½•")
print(f"  - {len(alerts)} æ¡é¢„è­¦")

mongo_client.close()
```

å®‰è£…ä¾èµ–å¹¶è¿è¡Œ:

```bash
pip install pymongo redis faker

python nosql_data_loader.py
```

---

## ğŸ“ Task 5: å®ç°5ä¸ªæŸ¥è¯¢

åˆ›å»º `nosql_queries.py`:

```python
from pymongo import MongoClient
import redis
import time
import json
from datetime import datetime, timedelta

# è¿æ¥
mongo_client = MongoClient("mongodb://admin:yourpassword@localhost:27017/")
db = mongo_client["token_analyzer"]
redis_client = redis.Redis(host='localhost', port=6379, decode_responses=True)

def benchmark_query(name, query_func, iterations=100):
    """æ‰§è¡ŒæŸ¥è¯¢å¹¶æµ‹é‡æ€§èƒ½"""
    # é¢„çƒ­
    query_func()

    # æ­£å¼æµ‹è¯•
    latencies = []
    for _ in range(iterations):
        start = time.time()
        results = query_func()
        latency = (time.time() - start) * 1000  # æ¯«ç§’
        latencies.append(latency)

    avg_latency = sum(latencies) / len(latencies)
    p95_latency = sorted(latencies)[int(len(latencies) * 0.95)]

    print(f"\n{name}")
    print(f"  å¹³å‡å»¶è¿Ÿ: {avg_latency:.2f}ms")
    print(f"  P95å»¶è¿Ÿ: {p95_latency:.2f}ms")
    print(f"  è¿”å›ç»“æœ: {len(results) if isinstance(results, list) else 'N/A'}")

    return avg_latency, p95_latency

# è·å–ç¤ºä¾‹tokenåœ°å€
sample_token = db.tokens.find_one()["address"]

# ============================================================================
# Q1: ç‚¹æŸ¥è¯¢ - è·å–å•ä¸ªä»£å¸æœ€æ–°ä¿¡æ¯ï¼ˆä½¿ç”¨Redisç¼“å­˜ï¼‰
# ============================================================================
def q1_query():
    # å…ˆæŸ¥Redis
    cache_key = f"token:{sample_token}:latest"
    cached = redis_client.get(cache_key)

    if cached:
        return json.loads(cached)

    # Redisæœªå‘½ä¸­ï¼ŒæŸ¥MongoDB
    result = db.tokens.find_one({"address": sample_token})

    # æ›´æ–°ç¼“å­˜
    cache_data = {
        "symbol": result["symbol"],
        "name": result["name"],
        "price": result["latest_price"]["dex_price"],
        "tvl": result["latest_price"]["tvl"],
        "score": result["latest_score"]["score"]
    }
    redis_client.setex(cache_key, 300, json.dumps(cache_data))

    return cache_data

benchmark_query("Q1: ç‚¹æŸ¥è¯¢ (å•ä¸ªä»£å¸ä¿¡æ¯ - Redisç¼“å­˜)", q1_query)

# ============================================================================
# Q2: èŒƒå›´æŸ¥è¯¢ - ä»·æ ¼å†å²èµ°åŠ¿
# ============================================================================
def q2_query():
    start_time = datetime.now() - timedelta(hours=24)
    results = list(db.dex_prices.find({
        "token_address": sample_token,
        "timestamp": {"$gte": start_time}
    }).sort("timestamp", -1))
    return results

benchmark_query("Q2: èŒƒå›´æŸ¥è¯¢ (24å°æ—¶ä»·æ ¼èµ°åŠ¿)", q2_query)

# ============================================================================
# Q3: èšåˆæŸ¥è¯¢ - Top 10é«˜è¯„åˆ†ä»£å¸
# ============================================================================
def q3_query():
    results = list(db.tokens.find().sort("latest_score.score", -1).limit(10))
    return results

benchmark_query("Q3: èšåˆæŸ¥è¯¢ (Top 10é«˜è¯„åˆ†ä»£å¸)", q3_query)

# ============================================================================
# Q4: å¤æ‚æŸ¥è¯¢ - æŒä»“é›†ä¸­åº¦åˆ†æï¼ˆåˆ©ç”¨é¢„è®¡ç®—å­—æ®µï¼‰
# ============================================================================
def q4_query():
    # åˆ©ç”¨å†…åµŒçš„top10_concentrationå­—æ®µ
    pipeline = [
        {"$match": {"top10_concentration": {"$gt": 30}}},
        {"$lookup": {
            "from": "tokens",
            "localField": "token_address",
            "foreignField": "address",
            "as": "token_info"
        }},
        {"$unwind": "$token_info"},
        {"$project": {
            "symbol": "$token_info.symbol",
            "top10_concentration": 1
        }},
        {"$sort": {"top10_concentration": -1}}
    ]
    results = list(db.token_holders.aggregate(pipeline))
    return results

benchmark_query("Q4: å¤æ‚æŸ¥è¯¢ (æŒä»“é›†ä¸­åº¦åˆ†æ)", q4_query, iterations=50)

# ============================================================================
# Q5: å†™å…¥æµ‹è¯• - æ‰¹é‡æ’å…¥ä»·æ ¼æ•°æ®
# ============================================================================
print("\nQ5: æ‰¹é‡å†™å…¥æµ‹è¯• (æ’å…¥1000æ¡ä»·æ ¼è®°å½•)")

test_data = [{
    "token_address": sample_token,
    "price": 100.0 + i*0.1,
    "tvl": 1000000.0,
    "liquidity_depth": 500000.0,
    "volume_24h": 100000.0,
    "timestamp": datetime.now(),
    "hour": datetime.now().replace(minute=0, second=0, microsecond=0)
} for i in range(1000)]

start = time.time()
db.dex_prices.insert_many(test_data)
write_time = (time.time() - start) * 1000

print(f"  æ‰¹é‡æ’å…¥1000æ¡: {write_time:.2f}ms")
print(f"  å¹³å‡æ¯æ¡: {write_time/1000:.2f}ms")

mongo_client.close()

print("\nâœ… NoSQLæŸ¥è¯¢æµ‹è¯•å®Œæˆï¼")
```

è¿è¡Œæµ‹è¯•:

```bash
python nosql_queries.py
```

---

## ğŸš€ Task 6: Redisç¼“å­˜ç­–ç•¥

### ç¼“å­˜çƒ­ç‚¹æ•°æ®

```python
# ç¼“å­˜ç­–ç•¥ç¤ºä¾‹
import redis
import json

redis_client = redis.Redis(host='localhost', port=6379, decode_responses=True)

# 1. ç¼“å­˜æœ€æ–°ä»·æ ¼ï¼ˆ5åˆ†é’ŸTTLï¼‰
def cache_latest_price(token_address, price_data):
    key = f"token:{token_address}:latest_price"
    redis_client.setex(key, 300, json.dumps(price_data))

# 2. ç¼“å­˜Top 10ä»£å¸ï¼ˆ10åˆ†é’ŸTTLï¼‰
def cache_top_tokens(top_tokens):
    key = "tokens:top10"
    redis_client.setex(key, 600, json.dumps(top_tokens))

# 3. ç¼“å­˜ä»£å¸è¯„åˆ†ï¼ˆ1å°æ—¶TTLï¼‰
def cache_token_score(token_address, score_data):
    key = f"token:{token_address}:score"
    redis_client.setex(key, 3600, json.dumps(score_data))
```

### ç¼“å­˜å¤±æ•ˆç­–ç•¥

```python
# å½“æ•°æ®æ›´æ–°æ—¶ï¼Œæ¸…é™¤ç›¸å…³ç¼“å­˜
def invalidate_cache(token_address):
    redis_client.delete(f"token:{token_address}:latest_price")
    redis_client.delete(f"token:{token_address}:score")
    redis_client.delete("tokens:top10")
```

---

## ğŸ“„ Task 7: æ–‡æ¡£å’Œäº¤ä»˜

åˆ›å»º `NOSQL_README.md`:

```markdown
# NoSQLæ–¹æ¡ˆå®ç°è¯´æ˜

## æ•°æ®åº“é…ç½®
- **æ–‡æ¡£æ•°æ®åº“**: MongoDB 7.0
- **ç¼“å­˜**: Redis 7.0
- **æ•°æ®è§„æ¨¡**:
  - 100ä¸ªä»£å¸
  - 10ä¸‡æ¡ä»·æ ¼è®°å½•
  - 2000æ¡æŒä»“è®°å½•ï¼ˆå†…åµŒæ–‡æ¡£ï¼‰

## Schemaè®¾è®¡è¦ç‚¹
1. **å†…åµŒæ–‡æ¡£**: å°†æœ€æ–°ä»·æ ¼å’Œè¯„åˆ†å†…åµŒåˆ°ä»£å¸ä¸»æ–‡æ¡£
2. **æ•°ç»„å­˜å‚¨**: æŒä»“è€…å­˜ä¸ºæ•°ç»„ï¼Œé¿å…JOIN
3. **é¢„è®¡ç®—å­—æ®µ**: top10_concentrationé¢„å…ˆè®¡ç®—
4. **Redisç¼“å­˜**: çƒ­ç‚¹æ•°æ®5åˆ†é’ŸTTL

## æ€§èƒ½ä¼˜åŠ¿
- ç‚¹æŸ¥è¯¢é€šè¿‡Redisç¼“å­˜ï¼Œå»¶è¿Ÿ<5ms
- å†…åµŒæ–‡æ¡£é¿å…JOINï¼Œå‡å°‘ç½‘ç»œå¾€è¿”
- æ‰¹é‡å†™å…¥æ€§èƒ½ä¼˜äºå…³ç³»å‹æ•°æ®åº“

## æ€§èƒ½æµ‹è¯•ç»“æœ

| æŸ¥è¯¢ | å¹³å‡å»¶è¿Ÿ | P95å»¶è¿Ÿ |
|------|---------|---------|
| Q1 ç‚¹æŸ¥è¯¢ (Redis) | XX ms | XX ms |
| Q2 èŒƒå›´æŸ¥è¯¢ | XX ms | XX ms |
| Q3 èšåˆæŸ¥è¯¢ | XX ms | XX ms |
| Q4 å¤æ‚æŸ¥è¯¢ | XX ms | XX ms |
| Q5 æ‰¹é‡å†™å…¥ | XX ms/1000æ¡ | - |

## æ–‡ä»¶æ¸…å•
- `nosql_indexes.js` - ç´¢å¼•åˆ›å»ºè„šæœ¬
- `nosql_data_loader.py` - æ•°æ®åŠ è½½è„šæœ¬
- `nosql_queries.py` - æŸ¥è¯¢æµ‹è¯•è„šæœ¬
- `docker-compose.yml` - Dockeréƒ¨ç½²é…ç½®
```

---

## âœ… äº¤ä»˜æ£€æŸ¥æ¸…å•

å®Œæˆåç¡®è®¤ï¼š

- [ ] MongoDBå’ŒRediså·²å®‰è£…å¹¶è¿è¡Œ
- [ ] 6ä¸ªé›†åˆåˆ›å»ºæˆåŠŸå¹¶åˆ›å»ºç´¢å¼•
- [ ] æ•°æ®åŠ è½½æˆåŠŸï¼ˆ100ä¸ªä»£å¸ï¼Œ10ä¸‡+æ¡è®°å½•ï¼‰
- [ ] Redisç¼“å­˜æ­£å¸¸å·¥ä½œ
- [ ] 5ä¸ªæŸ¥è¯¢éƒ½èƒ½æ­£å¸¸æ‰§è¡Œ
- [ ] è®°å½•äº†æ€§èƒ½æ•°æ®ï¼ˆå»¶è¿Ÿã€ååé‡ï¼‰
- [ ] æ–‡æ¡£å®Œæ•´

---

## ğŸ†˜ å¸¸è§é—®é¢˜

### Q: Dockerå®¹å™¨å¯åŠ¨å¤±è´¥ï¼Ÿ
A: æ£€æŸ¥ç«¯å£å ç”¨ï¼Œä½¿ç”¨ `docker-compose logs` æŸ¥çœ‹æ—¥å¿—

### Q: MongoDBè¿æ¥è¶…æ—¶ï¼Ÿ
A: æ£€æŸ¥è®¤è¯ä¿¡æ¯ï¼Œç¡®è®¤å®¹å™¨æ­£åœ¨è¿è¡Œ

### Q: Redisç¼“å­˜ä¸ç”Ÿæ•ˆï¼Ÿ
A: æ£€æŸ¥TTLè®¾ç½®ï¼Œä½¿ç”¨ `redis-cli` éªŒè¯æ•°æ®

### Q: æŸ¥è¯¢å¤ªæ…¢ï¼Ÿ
A: æ£€æŸ¥ç´¢å¼•æ˜¯å¦åˆ›å»ºæˆåŠŸï¼š`db.collection.getIndexes()`

---

**å®Œæˆæ—¶é—´**: Day 1-2ï¼Œå…±8å°æ—¶
**ä¸‹ä¸€æ­¥**: å°†æ€§èƒ½æ•°æ®äº¤ç»™å®éªŒç»„ç”¨äºå¯¹æ¯”åˆ†æ
